{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIMS probabilistic linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easylinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from easylinkage import preprocessing\n",
    "\n",
    "# Settings\n",
    "\n",
    "encoding = 'ISO-8859-1'\n",
    "\n",
    "# Variables\n",
    "\n",
    "INPUTDATA = \"../data/LIMS/probabilisticLinkage/\"\n",
    "\n",
    "TARGET_FILENAME = \"New lims extract.xlsx\"\n",
    "QUERY_FILENAME = \"LINC Gold Standard - Original Testing Registers\"\n",
    "\n",
    "# Reading functions\n",
    "\n",
    "def read_targetdata():\n",
    "    df = pd.read_excel(os.path.join(INPUTDATA, TARGET_FILENAME), sheet = \"Sheet 1\")\n",
    "    df = df[[\"PATID\", \"patnumber\", \"accessnumber\", \"FIRSTNAME\", \"NAME\", \"SEX\", \"birthdate\", \"LOCNAME\"]]\n",
    "    df = df.rename(dict(zip(df.columns, [\"patid1\", \"patid2\", \"patid3\", \"firstname\", \"surname\", \"sex\", \"birthdate\", \"locname\"])), axis = 1)\n",
    "    df = df.drop_duplicates()\n",
    "    # First Name\n",
    "    df.firstname = preprocessing.clean(df.firstname)\n",
    "    # Surname\n",
    "    df.surname = preprocessing.clean(df.surname)\n",
    "    # Sex\n",
    "    df.loc[df.sex == 1, [\"sex\"]] = \"m\"\n",
    "    df.loc[df.sex == 2, [\"sex\"]] = \"f\"\n",
    "    df.loc[df.sex == 0, [\"sex\"]] = np.nan\n",
    "    # Date\n",
    "    df.birthdate = pd.to_datetime(df[\"birthdate\"], errors = \"coerce\")\n",
    "    # Locname\n",
    "    df.locname = preprocessing.clean(df.locname)\n",
    "    df.locname = df.locname.str.replace('\\d+', '')\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def read_sourcedata():\n",
    "    return\n",
    "\n",
    "df = read_targetdata()\n",
    "dft = df[50:200].copy()\n",
    "dft = dft.reset_index(drop=True)\n",
    "dfq = df[:100].copy()\n",
    "dfq = dfq.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easylinkage import Index\n",
    "\n",
    "indexer = Index()\n",
    "indexer.softblock(left_on = [\"firstname\", \"surname\"], right_on = [\"firstname\", \"surname\"])\n",
    "pairs = indexer.index(dfq, dft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easylinkage import Compare\n",
    "\n",
    "comp = Compare()\n",
    "comp.name('firstname', 'firstname')\n",
    "comp.name('surname', 'surname')\n",
    "comp.birthdate(\"birthdate\", \"birthdate\")\n",
    "comp.sex(\"sex\", \"sex\")\n",
    "comp.location(\"locname\", \"locname\")\n",
    "\n",
    "dfc = comp.compute(pairs, dfq, dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f302ac998cce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0measylinkage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mECMClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mecm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mECMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Papers/cidrz/easylinkage/easylinkage/classifiers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, init, binarize, max_iter, atol, use_col_names, *args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m                  *args, **kwargs):\n\u001b[1;32m     15\u001b[0m         super(ECMClassifier, self).__init__(\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0muse_col_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_col_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[0;32m~/Documents/Papers/cidrz/easylinkage/easylinkage/classifiers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, init, binarize, max_iter, atol, use_col_names, *args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m                  *args, **kwargs):\n\u001b[1;32m     15\u001b[0m         super(ECMClassifier, self).__init__(\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0muse_col_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_col_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "from easylinkage import ECMClassifier\n",
    "\n",
    "ecm = ECMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easylinkage import ECMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleECMClassifier(ECMClassifier):\n",
    "     \n",
    "    def __init__(self, ensemble_size = 10, init = \"jaro\", max_iter = 100, binarize = None, atol = 1e-4, use_col_names = True):\n",
    "        ECMClassifier.__init__(self, init = init, max_iter = max_iter, binarize = binarize, atol = atol, use_col_names = use_col_names)\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.clfs = []\n",
    "        self.fit_columns = []\n",
    "        \n",
    "    def _sample_features(self, df):\n",
    "        def _get_feature_dict(df):\n",
    "            features = collections.defaultdict(list)\n",
    "            for c in df.columns:\n",
    "                features[c.split(\"---\")[0]] += [c]\n",
    "            return features\n",
    "        features = _get_feature_dict(df)\n",
    "        ensemble = []\n",
    "        for _ in range(0, self.ensemble_size):\n",
    "            feats = []\n",
    "            for k, v in features.items():\n",
    "                feats += [random.choice(v)]\n",
    "            ensemble += [feats]\n",
    "        return ensemble\n",
    "            \n",
    "    def fit(self, comparison_vectors):\n",
    "        dfc = comparison_vectors\n",
    "        ensemble = self._sample_features(dfc)\n",
    "        for cols in ensemble:\n",
    "            clf = ECMClassifier()\n",
    "            clf.fit(dfc[cols])\n",
    "            self.clfs += [clf]\n",
    "            self.fit_columns += [clf.raw_fit_columns]\n",
    "            \n",
    "    def fit_predict(self, comparison_vectors):\n",
    "        dfc = comparison_vectors\n",
    "        self.fit(dfc)\n",
    "        return self.predict(dfc)\n",
    "            \n",
    "    def predict(self, comparison_vectors):\n",
    "        dfc = comparison_vectors\n",
    "        prd = []\n",
    "        for cols, clf in zip(self.fit_columns, self.clfs):\n",
    "            prd += [clf.predict(dfc[cols])]\n",
    "        prd = pd.concat(prd, axis = 1)\n",
    "        prd = prd.mode(axis = 1)\n",
    "        return probas\n",
    "         \n",
    "    def prob(self, comparison_vectors):\n",
    "        dfc = comparison_vectors\n",
    "        prd = []\n",
    "        for cols, clf in zip(self.fit_columns, self.clfs):\n",
    "            prd += [clf.prob(dfc[cols])]\n",
    "        prd = pd.concat(prd, axis = 1)\n",
    "        prd = prd.median(axis = 1)\n",
    "        return prd\n",
    "    \n",
    "ecm = EnsembleECMClassifier()\n",
    "ecm.fit(dfc)\n",
    "prob = ecm.prob(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recordlinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.13.2+6.g2524fad'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recordlinkage.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uu = BaseECMClassifier(binarize = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecm = BaseECMClassifier(init = \"random\", binarize = 0.5)\n",
    "ecm.fit(uu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_dict(df):\n",
    "    features = collections.defaultdict(list)\n",
    "    for c in df.columns:\n",
    "        features[c.split(\"_\")[0]] += [c]\n",
    "    return features\n",
    "\n",
    "features = get_feature_dict(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 10\n",
    "\n",
    "def sample_features(df, B):\n",
    "    features = get_feature_dict(df)\n",
    "    ensemble = []\n",
    "    for _ in range(0, B):\n",
    "        feats = []\n",
    "        for k, v in features.items():\n",
    "            feats += [random.choice(v)]\n",
    "        ensemble += [feats]\n",
    "    return ensemble\n",
    "\n",
    "ensemble = sample_features(dfc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Probabilistic linkage\n",
    "\n",
    "ecm = classifiers.ECMClassifier(binarize = 0.5, max_iter = 1000, atol = 1e-10)\n",
    "ecm.fit(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecm.fit(comparison_vectors=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_proba = 0.0\n",
    "\n",
    "def raw_match(dfc):\n",
    "    sums = dfc.sum(axis=1)\n",
    "    return sums / dfc.shape[1]\n",
    "\n",
    "def ecm_probabilities(dfc):\n",
    "    ecm = classifiers.ECMClassifier(binarize = 0.5, max_iter = 1000, atol = 1e-10)\n",
    "    ecm.fit(dfc)\n",
    "    probas = ecm.prob(dfc)\n",
    "    return probas\n",
    "\n",
    "def ensemble_ecm_probabilities(dfc, ensemble):\n",
    "    probas = []\n",
    "    for cols in ensemble:\n",
    "        probas += [ecm_probabilities(dfc[cols])]\n",
    "    probas = pd.concat(probas, axis = 1)\n",
    "    probas = probas.median(axis = 1)\n",
    "    return probas\n",
    "\n",
    "def linkage(dfc, dfq, dft, ensemble):\n",
    "    raws  = raw_match(dfc)\n",
    "    probas = ensemble_ecm_probabilities(dfc, ensemble)\n",
    "    dfl = pd.DataFrame(data = {\"index.q\": probas.index.labels[0],\n",
    "                               \"index.t\": probas.index.labels[1],\n",
    "                               \"raw\": raws.values,\n",
    "                               \"proba\": probas.values,\n",
    "                               \"firstname.q\": dfq.iloc[probas.index.labels[0]][\"firstname\"].values,\n",
    "                               \"surname.q\": dfq.iloc[probas.index.labels[0]][\"surname\"].values,\n",
    "                               \"birthdate.q\": dfq.iloc[probas.index.labels[0]][\"birthdate\"].values,\n",
    "                               \"firstname.t\": dft.iloc[probas.index.labels[1]][\"firstname\"].values,\n",
    "                               \"surname.t\": dft.iloc[probas.index.labels[1]][\"surname\"].values,\n",
    "                               \"birthdate.t\": dft.iloc[probas.index.labels[1]][\"birthdate\"].values})\n",
    "    dfl = dfl[[\"index.q\", \"index.t\", \"raw\", \"proba\", \"firstname.q\", \"surname.q\", \"birthdate.q\", \"firstname.t\", \"surname.t\", \"birthdate.t\"]]\n",
    "    dfl = dfl[dfl.proba >= min_prob]\n",
    "    dfl = dfl.sort_values(by = [\"index.q\", \"raw\"], ascending = [True, False])\n",
    "    dfl = dfl.reset_index(drop = True)\n",
    "    return dfl\n",
    "\n",
    "dfl = linkage(dfc, dfq, dft, ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl.sort_values(by = \"proba\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = pd.concat(probas, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas.median(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.8\n",
    "\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n",
    "\n",
    "def get_lowest(df, cols):\n",
    "    low = 0\n",
    "    f1 = 0\n",
    "    f2 = 0\n",
    "    for k in itertools.combinations(cols, 2):\n",
    "        cos = spatial.distance.cosine(df[k[0]].values, df[k[1]].values)\n",
    "        if cos > low:\n",
    "            low = cos\n",
    "            f1 = k[0]\n",
    "            f2 = k[1]\n",
    "    return f1, f2\n",
    "\n",
    "def generate_pool(df, p):\n",
    "    features = defaultdict(list)\n",
    "    features_names = set([x.split(\"_\")[0] for x in df.columns])\n",
    "    for feature in sorted(features_names):\n",
    "        vf = [col for col in df.columns if feature in col]\n",
    "        if len(vf) == 1:\n",
    "            features[feature] = vf\n",
    "            continue\n",
    "        f1, f2 = get_lowest(df, vf)\n",
    "        V = [f1, f2]\n",
    "        features[feature].append(f1)\n",
    "        features[feature].append(f2)\n",
    "        vf.remove(f1)\n",
    "        vf.remove(f2)\n",
    "        while vf:\n",
    "            f = vf.pop()\n",
    "            c = 0\n",
    "            for item in V:\n",
    "                num = spatial.distance.cosine(df[f].values, df[item].values)\n",
    "                if num > p and f not in features:\n",
    "                    c += 1;\n",
    "            if c == len(V):\n",
    "                V.append(f)\n",
    "                features[feature].append(f)\n",
    "    return features\n",
    "\n",
    "def convert_to_features_schema(features):\n",
    "    features = list(dict(features).values())\n",
    "    features_schema = list(itertools.product(*features))\n",
    "    return features_schema\n",
    "\n",
    "def get_features_schema(df):\n",
    "    dfv = feature_vectors.reset_index(drop = True)\n",
    "    features = generate_pool(dfv, p)\n",
    "    features_schema = convert_to_features_schema(features)\n",
    "    return features_schema\n",
    "\n",
    "features_schema = get_features_schema(feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(df, features, Xm, Xu, w):\n",
    "    ls = []\n",
    "    djs = []\n",
    "    for index, f in enumerate(features):\n",
    "        match = (df.loc[Xm][f] - 1).sum(axis = 0)\n",
    "        notmatch = (df.loc[Xu][f] - 0).sum(axis = 0)\n",
    "        dj = match + notmatch\n",
    "        djs.append(dj)\n",
    "        if dj == 0:\n",
    "            ls.append(index)\n",
    "    if len(ls) > 0:\n",
    "        w = np.zeros(w.shape)\n",
    "        w[ls] = 1 / len(ls)\n",
    "    else :\n",
    "        s = sum(1 / d for d in djs)\n",
    "        djs = [round(1 / (dj*s), 5) for dj in djs]\n",
    "        w = np.asarray(djs).reshape(w.shape)\n",
    "    return w\n",
    "\n",
    "def feature_weights(df, Mm, Mu, e, w):\n",
    "    Xm=set()\n",
    "    Xu=set()\n",
    "    tm, tu = 0, 0\n",
    "    while(len(Xm)<Mm):\n",
    "        t=np.dot(abs(df[~df.index.isin(Xm)].values-1),w)\n",
    "        Xm.update(df[~df.index.isin(Xm)][t<=tm].head(Mm-len(Xm)).index) ### fill the seed until we reach Mm without repeating\n",
    "        tm+=0.05\n",
    "    while(len(Xu)<Mu):\n",
    "        t=np.dot(abs(df[~df.index.isin(Xu)].values-0),w)\n",
    "        inde=set(df[(~df.index.isin(Xu))][t<=tu].head(Mu-len(Xu)).index) ### make sure that no matching point is selected for not matching point\n",
    "        Xu.update(inde-Xm)\n",
    "        tu+=0.05\n",
    "    wnew=calculate_weights(df,df.columns,Xm,Xu,w)\n",
    "    while( np.array(abs(wnew-w)>e).any()):\n",
    "        Xm=set()\n",
    "        Xu=set()\n",
    "        tm,tu=0,0\n",
    "        w=wnew\n",
    "        while(len(Xm)<Mm):\n",
    "            t=np.dot(abs(df[~df.index.isin(Xm)].values-1),w)\n",
    "            Xm.update(df[~df.index.isin(Xm)][t<=tm].head(Mm-len(Xm)).index)\n",
    "            tm+=0.05\n",
    "        while(len(Xu)<Mu):\n",
    "            t=np.dot(abs(df[~df.index.isin(Xu)].values-0),w)\n",
    "            inde=set(df[(~df.index.isin(Xu))][t<=tu].head(Mu-len(Xu)).index)\n",
    "            Xu.update(inde-Xm)\n",
    "            tu+=0.05\n",
    "        wnew=calculate_weights(df,df.columns,Xm,Xu,w)\n",
    "    return Xm,Xu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl[dfl.proba > 0.5].sort_values(by = \"proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = classifiers.ECMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = rl.ECMClassifier(binarize=0.5, max_iter=100, atol = 1e-4)\n",
    "clf.fit(feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = feature_vectors.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = rl.ECMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?rl.ECMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.8\n",
    "\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n",
    "\n",
    "def get_lowest(df, cols):\n",
    "    low = 0\n",
    "    f1 = 0\n",
    "    f2 = 0\n",
    "    for k in itertools.combinations(cols, 2):\n",
    "        cos = spatial.distance.cosine(df[k[0]].values, df[k[1]].values)\n",
    "        if cos > low:\n",
    "            low = cos\n",
    "            f1 = k[0]\n",
    "            f2 = k[1]\n",
    "    return f1, f2\n",
    "\n",
    "def generate_pool(df, p):\n",
    "    features = defaultdict(list)\n",
    "    features_names = set([x.split(\"_\")[0] for x in df.columns])\n",
    "    for feature in sorted(features_names):\n",
    "        vf = [col for col in df.columns if feature in col]\n",
    "        if len(vf) == 1:\n",
    "            features[feature] = vf\n",
    "            continue\n",
    "        f1, f2 = get_lowest(df, vf)\n",
    "        V = [f1, f2]\n",
    "        features[feature].append(f1)\n",
    "        features[feature].append(f2)\n",
    "        vf.remove(f1)\n",
    "        vf.remove(f2)\n",
    "        while vf:\n",
    "            f = vf.pop()\n",
    "            c = 0\n",
    "            for item in V:\n",
    "                num = spatial.distance.cosine(df[f].values, df[item].values)\n",
    "                if num > p and f not in features:\n",
    "                    c += 1;\n",
    "            if c == len(V):\n",
    "                V.append(f)\n",
    "                features[feature].append(f)\n",
    "    return features\n",
    "\n",
    "def convert_to_features_schema(features):\n",
    "    features = list(dict(features).values())\n",
    "    features_schema = list(itertools.product(*features))\n",
    "    return features_schema\n",
    "\n",
    "def get_features_schema(df):\n",
    "    dfv = feature_vectors.reset_index(drop = True)\n",
    "    features = generate_pool(dfv, p)\n",
    "    features_schema = convert_to_features_schema(features)\n",
    "    return features_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv = feature_vectors.reset_index(drop = True)\n",
    "features = generate_pool(dfv, p)\n",
    "features_schema = convert_to_features_schema(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(df, features, Xm, Xu, w):\n",
    "    ls = []\n",
    "    djs = []\n",
    "    for index, f in enumerate(features):\n",
    "        match = (df.loc[Xm][f] - 1).sum(axis = 0)\n",
    "        notmatch = (df.loc[Xu][f] - 0).sum(axis = 0)\n",
    "        dj = match + notmatch\n",
    "        djs.append(dj)\n",
    "        if dj == 0:\n",
    "            ls.append(index)\n",
    "    if len(ls) > 0:\n",
    "        w = np.zeros(w.shape)\n",
    "        w[ls] = 1 / len(ls)\n",
    "    else :\n",
    "        s = sum(1 / d for d in djs)\n",
    "        djs = [round(1 / (dj*s), 5) for dj in djs]\n",
    "        w = np.asarray(djs).reshape(w.shape)\n",
    "    return w\n",
    "\n",
    "def automatic_seed_selection(df, Mm, Mu, e, w):\n",
    "    Xm=set()\n",
    "    Xu=set()\n",
    "    tm,tu=0,0\n",
    "    while(len(Xm)<Mm):\n",
    "        t=np.dot(abs(df[~df.index.isin(Xm)].values-1),w)\n",
    "        Xm.update(df[~df.index.isin(Xm)][t<=tm].head(Mm-len(Xm)).index) ### fill the seed until we reach Mm without repeating\n",
    "        tm+=0.05\n",
    "    while(len(Xu)<Mu):\n",
    "        t=np.dot(abs(df[~df.index.isin(Xu)].values-0),w)\n",
    "        inde=set(df[(~df.index.isin(Xu))][t<=tu].head(Mu-len(Xu)).index) ### make sure that no matching point is selected for not matching point\n",
    "        Xu.update(inde-Xm)\n",
    "        tu+=0.05\n",
    "    wnew=calculate_weights(df,df.columns,Xm,Xu,w)\n",
    "    while( np.array(abs(wnew-w)>e).any()):\n",
    "        Xm=set()\n",
    "        Xu=set()\n",
    "        tm,tu=0,0\n",
    "        w=wnew\n",
    "        while(len(Xm)<Mm):\n",
    "            t=np.dot(abs(df[~df.index.isin(Xm)].values-1),w)\n",
    "            Xm.update(df[~df.index.isin(Xm)][t<=tm].head(Mm-len(Xm)).index)\n",
    "            tm+=0.05\n",
    "        while(len(Xu)<Mu):\n",
    "            t=np.dot(abs(df[~df.index.isin(Xu)].values-0),w)\n",
    "            inde=set(df[(~df.index.isin(Xu))][t<=tu].head(Mu-len(Xu)).index)\n",
    "            Xu.update(inde-Xm)\n",
    "            tu+=0.05\n",
    "        wnew=calculate_weights(df,df.columns,Xm,Xu,w)\n",
    "    return Xm,Xu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mm = 200\n",
    "Mu = 8000\n",
    "e  = 0.5\n",
    "\n",
    "def calculate_Xm_Xu(df, features):\n",
    "    Xm = []\n",
    "    Xu = []\n",
    "    for k, v in features.items():\n",
    "        z = len(v)\n",
    "        w = np.full((z, 1), 1. / z)\n",
    "        x1, x2 = automatic_seed_selection(df[v], Mm, Mu, e, w)\n",
    "        Xm.append(x1)\n",
    "        Xu.append(x2)    \n",
    "    return Xm, Xu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xm, Xu = calculate_Xm_Xu(dfv, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL = set().union(*Xm)\n",
    "\n",
    "def calculate_Q(set0, set1):\n",
    "    S00 = set0.intersection(set1)\n",
    "    S11 = ALL - (set0.union(set1))\n",
    "    S01 = set0 - set1\n",
    "    S10 = set0 - set1\n",
    "    Q = float((len(S00)*len(S11)) - (len(S01) * len(S10))) / ((len(S00)*len(S11)) + (len(S01) * len(S10)))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qs = []\n",
    "for f in itertools.combinations(range(0,len(Xm)),2):\n",
    "    Qs.append((calculate_Q(Xm[f[0]],Xm[f[1]]),f[0],f[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frameworks.SelfLearning import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models = []\n",
    "X_features = []\n",
    "for i, schema in enumerate(features_schema):\n",
    "    model = SelfLearningModel(LogisticRegression(solver = \"lbfgs\"))\n",
    "    models.append(model)\n",
    "    X_features += [list(schema)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, model in enumerate(models):\n",
    "    X = dfv[X_features[i]].values\n",
    "    name = 'proba_' + str(i)\n",
    "    dfv[name] = -1\n",
    "    dfv.loc[Xm[i], name] = 1\n",
    "    dfv.loc[Xu[i], name] = 0\n",
    "    y = dfv[name].values\n",
    "    model.fit(X, y)\n",
    "    #dfv.loc[dfv[name] == -1, name] = model.predict_proba(dfv[dfv[name] == -1][X_features[i]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recordlinkage import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recordlinkage.standardise import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
